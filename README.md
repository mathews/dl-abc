
# Deep Learning Basic

## choosing activation fuction
1. While logistic sigmoid neurons are more bi-ologically plausible than hyperbolic tangent neurons, the latter work better for train-ing multi-layer neural networks. [Deep Sparse Rectifier Neural Networks](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf) 
2. [tanh vs ReLU](https://www.zhihu.com/question/61265076/answer/1136216727)

## average pooling
1. this blog gives a clear explanation, [Global average pooling--<Network In Network>](https://zhuanlan.zhihu.com/p/37683646)
2. [alexisbcook's professional explanation](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/)
3. [an explanation of average pooling with tensorflow codes](https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/)
4. [What are Max Pooling, Average Pooling, Global Max Pooling and Global Average Pooling?](https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/)


## Batch Normalization
1. [Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)

2. [深入理解Batch Normalization批标准化](https://www.cnblogs.com/guoyaohua/p/8724433.html)

## droupout
1. 


# Time Series forecasting

## LTSM


## LTSM-FCN


## Dataset

1. [public bicycle sharing data](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)
