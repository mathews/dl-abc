
# Deep Learning Basic

## choosing activation fuction
1. <Deep Sparse Rectifier Neural Networks>, While logistic sigmoid neurons are more bi-ologically plausible than hyperbolic tangent neurons, the latter work better for train-ing multi-layer neural networks. http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf, 
2. tanh vs ReLU, https://www.zhihu.com/question/61265076/answer/1136216727 

## average pooling
1. https://zhuanlan.zhihu.com/p/37683646
2. https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/
3. https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/
4. What are Max Pooling, Average Pooling, Global Max Pooling and Global Average Pooling? https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/


## Batch Normalization
1. Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift, https://arxiv.org/pdf/1502.03167.pdf



## Dataset

1. https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
