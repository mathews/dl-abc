
# Deep Learning Basic

## choosing activation fuction
1. [Deep Sparse Rectifier Neural Networks, While logistic sigmoid neurons are more bi-ologically plausible than hyperbolic tangent neurons, the latter work better for train-ing multi-layer neural networks.](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf) 
2. [tanh vs ReLU](https://www.zhihu.com/question/61265076/answer/1136216727)

## average pooling
1. [Global average pooling--<Network In Network>, this blog gives a clear explanation](https://zhuanlan.zhihu.com/p/37683646)
2. [alexisbcook's professional explanation](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/)
3. (https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/)
4. [What are Max Pooling, Average Pooling, Global Max Pooling and Global Average Pooling?](https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/)


## Batch Normalization
1. [Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)



## Dataset

1. [public bicycle sharing data](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)
